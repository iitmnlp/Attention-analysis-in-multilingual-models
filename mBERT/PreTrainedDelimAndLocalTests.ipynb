{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "StatTestsDelimAndLocal-Paper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcUrKUxW9Bo-"
      },
      "source": [
        "!pip install transformers==4.5.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtcdAQ_Y36zL"
      },
      "source": [
        "import transformers\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pickle\n",
        "import statistics \n",
        "from scipy import stats\n",
        "import math\n",
        "import csv"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdw7ib8y8FB1"
      },
      "source": [
        "weightsPath='/content/drive/My Drive/MTP/Weights/'\n",
        "datasetPath='/content/drive/My Drive/MTP/Datasets/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1igu-q-I4zsp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d0465a-dfe7-4b1c-d61b-efef1f1804dc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrqragbFPsuL"
      },
      "source": [
        "**Delimiter heads statistical test in pretrained mBERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pECRxv58d0wv"
      },
      "source": [
        "#This cell runs the statistical test for the delimiter roles (CLS,SEP) for the specified languages and functional roles.\n",
        "\n",
        "languages=['english']\n",
        "functions=['cls','sep']\n",
        "\n",
        "model='bert-base-multilingual-cased'\n",
        "MAX_LEN=64\n",
        "BATCH_SIZE=8\n",
        "\n",
        "for lang in languages:\n",
        "  for func in functions:\n",
        "    print(lang+' '+func)\n",
        "    sentences_file=datasetPath + lang + '-sentences-1000.txt'\n",
        "\n",
        "    in_file=open(sentences_file,'r',encoding='utf-8')\n",
        "\n",
        "    lines=in_file.readlines()\n",
        "\n",
        "    tokenizer=transformers.BertTokenizer.from_pretrained(model)\n",
        "\n",
        "    class SentenceDataset(Dataset):\n",
        "      \n",
        "      def __init__(self, sentences,tokenizer,max_len):\n",
        "        self.sentences=sentences\n",
        "        self.tokenizer=tokenizer\n",
        "        self.max_len=max_len\n",
        "\n",
        "      def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "      def __getitem__(self,item):\n",
        "        sentence=str(self.sentences[item])\n",
        "\n",
        "        encoding=tokenizer.encode_plus(\n",
        "            sentence,\n",
        "            max_length=self.max_len,\n",
        "            add_special_tokens=True,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=False,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return{\n",
        "            'sentence':sentence,\n",
        "            'input_ids':encoding['input_ids'],\n",
        "            'attention_mask':encoding['attention_mask']\n",
        "        }\n",
        "\n",
        "    n=len(lines)\n",
        "\n",
        "    def create_data_loader(sentences,tokenizer,max_len,batch_size):\n",
        "      ds= SentenceDataset(\n",
        "          sentences=sentences,\n",
        "          tokenizer=tokenizer,\n",
        "          max_len=max_len\n",
        "      )\n",
        "\n",
        "      return DataLoader(\n",
        "          ds,\n",
        "          batch_size=batch_size\n",
        "      )\n",
        "\n",
        "\n",
        "    data_loader=create_data_loader(lines,tokenizer,MAX_LEN,BATCH_SIZE)\n",
        "\n",
        "    if (func=='cls'):\n",
        "      index=0\n",
        "    else:\n",
        "      index=-1\n",
        "\n",
        "    headSentScores=[[0 for i in range(1000)] for j in range(145)]\n",
        "\n",
        "    bert_model = transformers.BertModel.from_pretrained(model, output_attentions=True)\n",
        "\n",
        "    for count, data in enumerate(iter(data_loader)):\n",
        "        last_hidden_state,pooler_output,attentions = bert_model(input_ids=torch.squeeze(data['input_ids']),\n",
        "                                                                  attention_mask=torch.squeeze(data['attention_mask']),return_dict=False)\n",
        "\n",
        "        for attention_id, attention in enumerate(attentions):\n",
        "            for element in range(attention.shape[0]):\n",
        "              tokens = tokenizer.tokenize(lines[count * BATCH_SIZE + element])\n",
        "              for head in range(attention.shape[1]):\n",
        "                  number = attention_id * 12 + (head + 1)\n",
        "                  sentenceWeights = attention[element][head]\n",
        "                  final_tokens = ['CLS'] + tokens[:62] + ['SEP']\n",
        "                  sent_len = len(final_tokens)\n",
        "                  array = sentenceWeights[:sent_len, :sent_len].detach().numpy()\n",
        "                  sentScore=0\n",
        "                  for i in range(sent_len):\n",
        "                      tokenScore=array[i,index]/(array[i,:].sum()/sent_len)\n",
        "                      sentScore+=tokenScore\n",
        "                  sentScore/=sent_len\n",
        "                  headSentScores[number][count * BATCH_SIZE + element]=sentScore\n",
        "                    \n",
        "        print(count)\n",
        "\n",
        "    # with open(weightsPath + lang +'-matrix-' + func + '-1000.pl', 'wb') as f:\n",
        "    #   pickle.dump(headSentScores, f)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaZWF-jNUAaX"
      },
      "source": [
        "**Local heads statistical test in pretrained mBERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-e1iGQ2hLtR"
      },
      "source": [
        "#This cell runs the statistical tests for the local role for the specified languages.\n",
        "\n",
        "languages=['english']\n",
        "functions=['local']\n",
        "\n",
        "model='bert-base-multilingual-cased'\n",
        "bert_model = transformers.BertModel.from_pretrained(model, output_attentions=True)\n",
        "threshold=3\n",
        "MAX_LEN=64\n",
        "BATCH_SIZE=8\n",
        "\n",
        "for lang in languages:\n",
        "  for func in functions:\n",
        "    print(lang+' '+func)\n",
        "    sentences_file=datasetPath + lang + '-sentences-1000.txt'\n",
        "\n",
        "    in_file=open(sentences_file,'r',encoding='utf-8')\n",
        "\n",
        "    lines=in_file.readlines()\n",
        "\n",
        "    tokenizer=transformers.BertTokenizer.from_pretrained(model)\n",
        "\n",
        "    class SentenceDataset(Dataset):\n",
        "      \n",
        "      def __init__(self, sentences,tokenizer,max_len):\n",
        "        self.sentences=sentences\n",
        "        self.tokenizer=tokenizer\n",
        "        self.max_len=max_len\n",
        "\n",
        "      def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "      def __getitem__(self,item):\n",
        "        sentence=str(self.sentences[item])\n",
        "\n",
        "        encoding=tokenizer.encode_plus(\n",
        "            sentence,\n",
        "            max_length=self.max_len,\n",
        "            add_special_tokens=True,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=False,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return{\n",
        "            'sentence':sentence,\n",
        "            'input_ids':encoding['input_ids'],\n",
        "            'attention_mask':encoding['attention_mask']\n",
        "        }\n",
        "\n",
        "    n=len(lines)\n",
        "\n",
        "    def create_data_loader(sentences,tokenizer,max_len,batch_size):\n",
        "      ds= SentenceDataset(\n",
        "          sentences=sentences,\n",
        "          tokenizer=tokenizer,\n",
        "          max_len=max_len\n",
        "      )\n",
        "\n",
        "      return DataLoader(\n",
        "          ds,\n",
        "          batch_size=batch_size\n",
        "      )\n",
        "\n",
        "\n",
        "    data_loader=create_data_loader(lines,tokenizer,MAX_LEN,BATCH_SIZE)\n",
        "\n",
        "    headSentScores=[[0 for i in range(1000)] for j in range(145)]\n",
        "\n",
        "    for count, data in enumerate(iter(data_loader)):\n",
        "        last_hidden_state,pooler_output,attentions = bert_model(input_ids=torch.squeeze(data['input_ids']),\n",
        "                                                                  attention_mask=torch.squeeze(data['attention_mask']),return_dict=False)\n",
        "        for attention_id, attention in enumerate(attentions):\n",
        "            for element in range(attention.shape[0]):\n",
        "              tokens = tokenizer.tokenize(lines[count * BATCH_SIZE + element])\n",
        "              for head in range(attention.shape[1]):\n",
        "                  number = attention_id * 12 + (head + 1)\n",
        "                  sentenceWeights = attention[element][head]\n",
        "                  final_tokens = ['CLS'] + tokens[:62] + ['SEP']\n",
        "                  sent_len = len(final_tokens)\n",
        "                  array = sentenceWeights[:sent_len, :sent_len].detach().numpy()\n",
        "                  sentScore=0\n",
        "                  counti=0\n",
        "                  for i in range(sent_len):\n",
        "                      if((i-2)>=0 and i+2<sent_len):\n",
        "                          counti+=1\n",
        "                          tokenScore=0\n",
        "                          for j in range(-2,3):\n",
        "                              tokenScore+=array[i,i+j]\n",
        "                          tokenScore=(tokenScore/5)/(array[i,:].sum()/sent_len)\n",
        "                          sentScore+=tokenScore\n",
        "                  if(counti!=0):\n",
        "                      sentScore/=counti\n",
        "                  headSentScores[number][count * BATCH_SIZE + element]=sentScore\n",
        "                    \n",
        "        print(count)\n",
        "\n",
        "    # with open(weightsPath + lang +'-matrix-' + func + '-1000.pl', 'wb') as f:\n",
        "    #   pickle.dump(headSentScores, f)"
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}