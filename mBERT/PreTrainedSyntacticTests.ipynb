{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StatTestsSyntactic-Paper",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7SPnAzLcH4U"
      },
      "source": [
        "!pip install transformers==4.5.1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5MlD8MFJaOM"
      },
      "source": [
        "import transformers\n",
        "import statistics \n",
        "from scipy import stats\n",
        "import math\n",
        "import pickle\n",
        "import statistics\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWTpM54_W-PR"
      },
      "source": [
        "ptweightsPath='/content/drive/My Drive/MTP/Weights/'\n",
        "datasetPath='/content/drive/My Drive/MTP/Datasets/'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZNvvxY8cZz7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420ddf7e-26fd-4906-cefb-598bd238ef28"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jz8tywHh8NR"
      },
      "source": [
        "#This cell runs the statistical tests for the syntactic functional roles in the specified languages.\n",
        "\n",
        "languages=['english']\n",
        "functions=['nsubj']\n",
        "\n",
        "model='bert-base-multilingual-cased'\n",
        "MAX_LEN=64\n",
        "BATCH_SIZE=8\n",
        "threshold=3\n",
        "\n",
        "bert_model = transformers.BertModel.from_pretrained(model, output_attentions=True)\n",
        "\n",
        "for lang in languages:\n",
        "  for func in functions:\n",
        "    print(lang,' ',func)\n",
        "    sentences_file=datasetPath + lang + '-sentences-' + func + '-1000.txt'\n",
        "    tokens_file=datasetPath + lang + '-' + func + '-1000-new.txt'\n",
        "    in_file=open(sentences_file,'r',encoding='utf-8')\n",
        "\n",
        "    lines=in_file.readlines()\n",
        "\n",
        "    tokenizer=transformers.BertTokenizer.from_pretrained(model)\n",
        "\n",
        "    class SentenceDataset(Dataset):\n",
        "      \n",
        "      def __init__(self, sentences,tokenizer,max_len):\n",
        "        self.sentences=sentences\n",
        "        self.tokenizer=tokenizer\n",
        "        self.max_len=max_len\n",
        "\n",
        "      def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "      def __getitem__(self,item):\n",
        "        sentence=str(self.sentences[item])\n",
        "\n",
        "        encoding=tokenizer.encode_plus(\n",
        "            sentence,\n",
        "            max_length=self.max_len,\n",
        "            add_special_tokens=True,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=False,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return{\n",
        "            'sentence':sentence,\n",
        "            'input_ids':encoding['input_ids'],\n",
        "            'attention_mask':encoding['attention_mask']\n",
        "        }\n",
        "    \n",
        "    n=len(lines)\n",
        "\n",
        "    file=open(tokens_file,\"r\",encoding=\"utf-8\")\n",
        "\n",
        "    sentences=file.read().splitlines()\n",
        "\n",
        "    filteredLines=[]\n",
        "    filteredIndices=[]\n",
        "    filteredTokens=[]\n",
        "    for i in range(0,len(sentences),2):\n",
        "      filteredLines.append(lines[int(sentences[i].split()[0])-1])\n",
        "      filteredIndices.append(sentences[i].split(' ')[1:])\n",
        "      filteredTokens.append(sentences[i+1].split())\n",
        "\n",
        "    \n",
        "    def create_data_loader(sentences,tokenizer,max_len,batch_size):\n",
        "      ds= SentenceDataset(\n",
        "          sentences=sentences,\n",
        "          tokenizer=tokenizer,\n",
        "          max_len=max_len\n",
        "      )\n",
        "\n",
        "      return DataLoader(\n",
        "          ds,\n",
        "          batch_size=batch_size\n",
        "      )\n",
        "\n",
        "\n",
        "    data_loader=create_data_loader(filteredLines,tokenizer,MAX_LEN,BATCH_SIZE)\n",
        "\n",
        "    def wordToTokenMappings(line):\n",
        "  \n",
        "      encoding=[tokenizer.encode_plus(\n",
        "            x,\n",
        "            add_special_tokens=False,\n",
        "            return_token_type_ids=None\n",
        "        )['input_ids'] for x in line]\n",
        "\n",
        "      idx=1\n",
        "      desired_output=[]\n",
        "\n",
        "      for token in encoding:\n",
        "        tokenoutput = []\n",
        "        for ids in token:\n",
        "          tokenoutput.append(idx)\n",
        "          idx +=1\n",
        "        desired_output.append(tokenoutput)\n",
        "\n",
        "      return desired_output\n",
        "\n",
        "    headSentScores=[[0 for i in range(len(filteredLines))] for j in range(145)]\n",
        "\n",
        "    for count, data in enumerate(iter(data_loader)):\n",
        "        last_hidden_state,pooler_output,attentions = bert_model(input_ids=torch.squeeze(data['input_ids'],dim=1),\n",
        "                                                                  attention_mask=torch.squeeze(data['attention_mask'],dim=1),return_dict=False)\n",
        "        for attention_id, attention in enumerate(attentions):\n",
        "            for element in range(attention.shape[0]):\n",
        "              tokens = tokenizer.tokenize(filteredLines[count * BATCH_SIZE + element])\n",
        "              mappings=wordToTokenMappings(filteredTokens[count * BATCH_SIZE + element])\n",
        "              indices=filteredIndices[count * BATCH_SIZE + element]\n",
        "              for head in range(attention.shape[1]):\n",
        "                  number = attention_id * 12 + (head + 1)\n",
        "                  sentenceWeights = attention[element][head]\n",
        "                  final_tokens = ['CLS'] + tokens[:62] + ['SEP']\n",
        "                  sent_len = len(final_tokens)\n",
        "                  array = sentenceWeights[:sent_len, :sent_len].detach().numpy()\n",
        "                  sentScore=0\n",
        "                  tokenCount=0\n",
        "                  for index in indices:\n",
        "                    dependent,head=index.split(',')\n",
        "                    dependent=int(dependent)\n",
        "                    head=int(head)\n",
        "                    dependentTokens=mappings[dependent-1]\n",
        "                    headTokens=mappings[head-1]\n",
        "                    for i in dependentTokens:\n",
        "                      tokenCount+=1\n",
        "                      tokenScore=0\n",
        "                      countHeads=0\n",
        "                      for j in headTokens:\n",
        "                        countHeads+=1\n",
        "                        tokenScore+=array[i,j]\n",
        "                      tokenScore=(tokenScore/countHeads)/(array[i,:].sum()/sent_len)\n",
        "                      sentScore+=tokenScore\n",
        "                  sentScore/=tokenCount\n",
        "                  headSentScores[number][count * BATCH_SIZE + element]=sentScore\n",
        "                    \n",
        "        print(count)\n",
        "\n",
        "    # with open(ptweightsPath + lang + '-matrix-' + func + '-1000.pl', 'wb') as f:\n",
        "    #   pickle.dump(headSentScores, f)"
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}